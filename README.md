# Naïve Bayes for Text Classification

# Introduction no Naïve Bayes

❑ Naive Bayes is a classification algorithm based on Bayes' theorem.

❑ It assumes that features are conditionally independent of each other given the class label.

❑ It builds a probability model by estimating the probabilities of features given each class label.

❑ Naive Bayes is primarily used for classification tasks and predicts the class label with the highest probability.

❑ It can handle both categorical and numerical features.

❑ Laplace smoothing is often applied to avoid zero probabilities.

❑ Naive Bayes is computationally efficient and requires a small amount of training data.

❑ It is commonly used in text classification tasks such as spam filtering and sentiment analysis.

❑ There are different variants of Naive Bayes, such as Multinomial Naive Bayes and Gaussian Naive Bayes.

❑ Naive Bayes can still perform well even if the independence assumption is violated to some extent.

# Basic Probability

![image](https://github.com/TITHI-KHAN/Naive-Bayes-for-Text-Classification/assets/65033964/39921251-27a0-4432-b3b6-53abaa5f6e2d)

# Bayesian Classifier 

**Notation:**

![image](https://github.com/TITHI-KHAN/Naive-Bayes-for-Text-Classification/assets/65033964/13fcf964-621e-4d73-8f31-17964706d4bc)

Bayesian classifiers have been used in a wide range of applications, including email spam filtering, medical diagnosis, image recognition, and natural language processing.

**Application:**

![image](https://github.com/TITHI-KHAN/Naive-Bayes-for-Text-Classification/assets/65033964/9b7f389f-99b9-407a-bd04-e2887b7a0316)

**Bayes rule:**

Bayes' rule, also known as Bayes' theorem or Bayes' law, is a fundamental concept in probability theory. It describes how to update or revise the probability of an event based on new evidence or information. Mathematically, Bayes' rule is represented as:

**P(A|B) = (P(B|A) * P(A)) / P(B)**

In this equation:

▪ P(A|B) is the posterior probability of event A given evidence B. It represents the probability of event A occurring given that evidence B is true.

▪ P(B|A) is the conditional probability of evidence B given event A. It represents the probability of observing evidence B when event A is true.

▪ P(A) is the prior probability of event A. It represents the initial or prior probability of event A before considering any evidence.

▪ P(B) is the probability of evidence B. It represents the overall probability of observing evidence B, regardless of event A.

![image](https://github.com/TITHI-KHAN/Naive-Bayes-for-Text-Classification/assets/65033964/3265a900-2d11-4bf3-b3ac-06d84b5139b0)

**Optimality:**

![image](https://github.com/TITHI-KHAN/Naive-Bayes-for-Text-Classification/assets/65033964/20a19295-ff79-49d2-aca1-5cc778313af3)

![image](https://github.com/TITHI-KHAN/Naive-Bayes-for-Text-Classification/assets/65033964/82e29456-1707-49ec-8acb-cb2124e5e172)

# Naïve Bayes: Problems

![image](https://github.com/TITHI-KHAN/Naive-Bayes-for-Text-Classification/assets/65033964/60bb12a9-6529-49a8-b168-d3bb5ea736e4)

![image](https://github.com/TITHI-KHAN/Naive-Bayes-for-Text-Classification/assets/65033964/a42e2c59-2b7d-4470-879b-df854bffb16d)

![image](https://github.com/TITHI-KHAN/Naive-Bayes-for-Text-Classification/assets/65033964/d54cf118-cfed-49f9-8caf-6a73089389d0)

![image](https://github.com/TITHI-KHAN/Naive-Bayes-for-Text-Classification/assets/65033964/e3c48ce2-a78a-4506-9e61-99a8b02adf8c)

![image](https://github.com/TITHI-KHAN/Naive-Bayes-for-Text-Classification/assets/65033964/67911423-7fd8-4c45-b455-de0ba10a3cbf)

![image](https://github.com/TITHI-KHAN/Naive-Bayes-for-Text-Classification/assets/65033964/7bc98eee-e6a8-4188-b79a-b2a85a086242)

![image](https://github.com/TITHI-KHAN/Naive-Bayes-for-Text-Classification/assets/65033964/f13774b8-c1a1-4cc7-a3f8-722274e02fb0)

![image](https://github.com/TITHI-KHAN/Naive-Bayes-for-Text-Classification/assets/65033964/e4da0c0c-9558-4b37-b4c2-96f4edaf18a8)


